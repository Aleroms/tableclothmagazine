It has recently come to my attention that despite their importance in technical
interviews (and a little bit related to programming as a whole), fundamental
algorithms are not required learning for UCI’s GDIM or Informatics majors. I will
provide here a brief explanation of when to use each technique and the intuition
behind how they work.

## Divide and Conquer

Divide and conquer algorithms use recursion to split a hard problem into easier
subproblems, “conquer” or find a solution for those subproblems, then bring
those pieces back together to get a solution for the original input.

The steps to formulate a divide
and conquer algorithm are as
follows:

1. Identify the base recursive
   case, which is the simplest
   possible subproblem.
2. Recursively split the input
   and solve each section.
3. Merge the solved splits.

Here are some common problems that
can be solved with a divide and conquer
approach:

- Sorting arrays
- Problems involving binary trees, in
  the event that you have to recurse
  into a branch of the tree.
- Syntax parsing, such as solving a
  long math equation by splitting it
  into smaller parts

I’m going to use Merge sort to explain how a divide and conquer algorithm
works. If you already know how merge sort works, then that is good! You already
know how to implement one example of a divide and conquer algorithm and can
use that intuition for making solutions for similar problems.

Let’s imagine for a moment that my name is John von Neumann and I’m the
smartest person on the planet and some guy comes up to me and tells me to sort
their array. Now, imagine that their array is super huge and has a lot of numbers
and that I can’t be bothered to spend the whole day sorting it by repeatedly
looking over the whole array trying to find the smallest number, then the second
smallest number, and so on…

Instead what I’m going to do is split that thing in half, so now I only have to sort
two arrays that are half as long. It’s still going to take me some time to sort each
half, but at least it’s less than what I was given at the start. In fact, I can just keep
splitting the array into halves, then quarters, then eighths, each time making the
problem easier to solve. At some point however, the fun is over because I can no
longer split the array in half. This will be when the subarray has only one
number, because I can’t split an array of one element in half no matter how
smart I am. BUT, there is a silver lining because if I have an array of one element,
then that array is already solved because a one-element array is sorted no
matter what. That was step 1 of the formula above: the base case of merge sort is
when the array you’re given has only one element.

Okay, so now that we have the base case, and we’ve already committed to
splitting our array in half each time (that’s step 2), we now need to figure out
how to combine two sorted subarrays for step 3. Here’s what’s going to happen:
we’re going to repeatedly compare the smallest elements of each of the two
subarrays. Whichever of those two elements is smallest is going to be taken from
the subarray and put it in our merged, sorted array. Repeat this until all the
elements are in the merged array, and it will be in sorted order.

```python
int[] MergeSort(int[] input):
# 1. Define the base case
if input is 1 or 0 elements:
# An array with only one element is always sorted by
# default, so just return it.
return input;
# 2. Split and recurse
sortedLeftHalf = MergeSort(left half of the input);
sortedRightHalf = MergeSort(right half of the input);
# 3. Merge the solved halves back together
sortedResult = [];
while (sortedLeftHalf and sortedRightHalf are not empty):
Take the smaller number of either sortedLeftHalf or
sortedRightHalf and put it in sortedResult.
```

Split up the input until
you get just single
elements, then merge
the subarrays back
together in sorted
order.

![mergesort](/issues/2/article/9/mergesort.png)

Now I can give the array back to the guy who asked me to sort it after only
spending `O(n log n)`. `O(n log n)` time on it because I’m John von Neumann and I’m
the smartest person ever. Great!

## Aside: What is the runtime of a divide and conquer algorithm?

How did we get that merge sort is `O(n log n)`? It’s
sort of complicated because the algorithm needs
to iterate over all n elements, then iterate over
`n/2` elements twice, then `n/4` elements four
times, and so on. We solve this using something
called the master theorem1 , which tells us the
runtime of any recursive algorithm. However,
finding the runtime of a divide and conquer
algorithm, although useful, doesn’t come up often
in interviews, so I will skip a detailed explanation
of it. You can check it out on your own if you
would like.

## Dynamic Programming

Recursion is cool and all, but doing it a lot could make your program super slow.
This is sometimes because recursion causes an algorithm to do repeated work,
where separate recursive calls compute the same output. Dynamic programming
is a method that avoids that repeated work by keeping track of each of those
values as they’re computed. Here are the steps to make a dynamic programming
algorithm:

1. Identify the base case.
2. Identify the recurrence relation, which says how we can solve step n given
   what we computed in the previous n-1 steps.
3. Memoize (write down in an array) the results of each step of the problem.

Let’s say that someone comes up to me and asks me to make an algorithm that
gives them the nth Fibonacci number: the number series where each element is
the sum of the previous two (0, 1, 1, 2, 3, 5, 8, etc.). Now I - thinking that I’m still
John von Neumann and am super smart - will write a very simple and elegant
recursive solution:

[Master Theorem](<https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)>)

```python
# Returns the nth Fibonacci number
int Fib(int n):
if (n < 2):
return n;
else:
return Fib(n-1) + Fib(n-2);
```

Look at that! Very simple, and it works. Then, someone will ask me to run it with
some stupid large number like n = 1000 and my computer will explode. Clearly,
recursion is not the best way to do this. To get some intuition about how we can
improve this, let’s look at a graph of all the calls to Fib when we input n = 5.

![recursion](/issues/2/article/9/recursion.png)

So, all the calls that are the same color share the same input. What’s interesting
is that since Fib will always produce the same output for a given input (meaning
there are no side effects), then each call to Fib(n) after the first is repeated work
for each value of n. That is lame. What if instead, we just remembered what the
result of each unique call to Fib gave us, and used that on subsequent calls to Fib
with the same input? For example, in the diagram, we could just call Fib(2) once,
then for the other two times, just remember what the answer to the first call was
instead of having to call Fib(2) and then Fib(1) and Fib(0) again.

The idea of remembering the result of a function call is called “memoization”.
This is done by storing the results of the function calls in an array, which is
referred to as a “memo” or “memoization array”. In this case, it’ll be like the 0th
element of our memoization array is the 1st Fibonacci number, the 1st element is
the 2nd Fibonacci number, and so on.

Now that we have all the tools we need to make a dynamic programming
algorithm, let’s try to solve our problem.

Referring to the steps listed before, what’s the base case for this problem? In
general, dynamic programming is a way of converting a recursive algorithm into
a more linear one. Therefore, the answer to step 1 should just be the base
recursive case. In this example, that’s when n is either 0 or 1. We’ll implement
this by initializing our memoization array to the values [0, 1].

Now for step 2: How does the nth Fibonacci number relate to the ones that come
before it? We can get this by looking at the equation for computing Fibonacci
numbers, which says that the nth Fibonacci number is the sum of the two which
come before it. We’ll compute Fib(n) by looking at the memoization array and
adding together the elements at [n-1] and [n-2]. Store this result in the
memoization array for step 3, and we should be good to go.
Let’s take a look at the algorithm:

```python
int Fib(n):
# 1. Initialize with the base case.
int[] memoizationArray = [0,1];
# 2. Compute each subsequent solution using the previously
# computed solutions.
for (int i = 2 to n):
int ithFibonacciNumber = memoizationArray[i-1] +
memoizationArray[i-2];
# 3. Memoize our intermediate result.
memoizationArray.push(ithFibonacciNumber);
return memoizationArray[n-1];
```

Look at that! We were able to take an exponential-time recursive algorithm and
turn it into a linear-time algorithm. Beautiful. Some of the more challenging
dynamic programming problems will involve using a 2D memoization array, so
just be aware that the array isn’t limited to one dimension. Here are some other
common problems that can be solved with dynamic programming:
● Maximizing or minimizing a value.
● Problems involving getting a subsequence of an array.
● Getting the edit distance between two strings.
● Knapsack problems, involving swapping out items to maximize value.
The Fibonacci numbers problems is sort of a bad example because you don’t
actually need to use `O(n)` memory for the memoization array. I’ll leave it up to
you to find the algorithm that uses constant memory.

## Aside: What the heck does “dynamic programming” mean, anyways?

**Short answer**: It sounds cool.

**Long answer**: There are varying accounts of how the name came about.

The guy who is attributed with coming up with dynamic programming - Richard Bellman - wrote a book some time ago in which he briefly talks about the name. While he
was working on this technique, his boss’s boss’s boss (or something like that) was
the Secretary of Defense, who had a literal fear of anything that was research or
research-adjacent, like mathematics. So, Richard came up with “dynamic
programming” to sort of talk about what he was doing without making it seem
like he was a scientist of some sort. However, the first paper using the term
“dynamic programming” came out before the guy he said was Secretary of
Defense was actually the Secretary of Defense. Some other guy who knew about
this situation at the time later stated in an interview that the reason why Richard
came up with “dynamic programming” was to one-up this other dude at his
company who made a technique called “linear programming”. Either way, both
stories would agree that “dynamic programming” sounds more interesting than
whatever else it could have possibly been called.

Sources if you want them (just what is listed on Wikipedia):

- [Bellman, R (1984)](https://web.archive.org/web/20141019015133/http://a2c2.org/awards/richard-e-bellman-control-heritage-award/2004-00-00t000000/harold-j-kushner)
  Eye of the hurricane: An autobiography. World Scientific.

- Russell, S.; Norvig, P. (2009). Artificial Intelligence: A Modern Approach (3rd
  ed.). Prentice Hall.

## Greedy

Suppose now that I am just me this time, and I am playing a game where my
inventory is limited by some max weight that I can carry. In this game, I have
come across a situation where there are several piles of loot before me. Each pile
consists of one type of precious material split into little pieces, such as
diamonds, gold coins, silver coins, and so on. Each pile also weighs a certain
amount, and unfortunately, the total weight of all the piles exceeds my
inventory’s weight limit. So, my problem is to choose which items to pick up, and
how much of each pile I should loot.

Because I am greedy and don’t want to think too hard, I will come up with a
simple algorithm for choosing what to pick up:

```python
void GreedyLoot(Pile[] itemsToLoot):
# A Pile is a tuple of the total value of that pile and the
# total weight of the pile.
while (my inventory is not full):
Pick up as much of the most valuable pile as I can.
Remove that pile from itemsToLoot.
```

What makes one pile more valuable than another? We’ll calculate this using the
value of that pile divided by the weight of the pile. Since our goal is to maximize
the value of all the items in our inventory, but the amount of stuff we can pick up
is limited by the weight capacity of our inventory, then we should always be
looking to fill the inventory with items for which one weight unit of them (for
instance, one pound) is worth more than one weight unit of any other available
item. We can think of this as the “value density” of the objects in the pile, where
objects that are more “dense” have a greater value-per-weight ratio. If it doesn’t
make sense why the value-per-weight of a pile is a good metric, don’t worry:
we’ll go over later why any other metric can’t possibly give you a better result.

Okay, so now I’m going to claim that this algorithm produces an optimal solution.
In other words, if I pick up items according to this algorithm, I will end up the
richest I could possibly be in this situation.

How do I know this? Well, suppose someone else follows some other algorithm
and ends up with an inventory that looks different than the one that this solution
gives me. That would mean that the other inventory has some amount of an item
that is different from the one in the greedy inventory. You can think of it like
some of the loot in the greedy inventory was “swapped out” for another type of
loot in the alternate inventory. Since the greedy algorithm always chooses the
loot that has the highest value-to-weight ratio at any given step, that means the
alternate algorithm at some point must have chosen to pick up an item that
didn’t have the highest value-to-weight ratio at the time. The item that the
greedy inventory didn’t pick up is what makes it different from the alternate
inventory, and since that loot is less valuable, that means the greedy inventory is
better than any possible alternate inventory. Therefore, the greedy inventory is
optimal!

![greedy](/issues/2/article/9/greedy1.png)

![greedy](/issues/2/article/9/greedy2.png)

So that’s a lot of words, but what is the method for forming a greedy algorithm?
Well it turns out that it’s kind of hard to define. Here’s some intuition:

1. A greedy algorithm is when a locally optimal decision leads to a globally
   optimal solution.

What does that mean? In this example, the “locally optimal decision” was at any
given time, choose the most valuable item and loot as much of it as I can. The
“globally optimal solution” was an inventory worth more than any other possible
inventory in this situation. Another way to think of this is to see if there is one
simple rule that you can follow at each iteration or sub-problem that will
eventually lead you to the correct solution. Here are some examples of problems
with greedy solutions:

● Fractional knapsack (that’s what the problem we did is called)
● Pathfinding (Dijkstra’s algorithm)
● Forming trees (Prim’s, Boruvka’s, Kruskal’s algorithms are greedy)
● Interval scheduling
● Graph coloring (Has a greedy heuristic algorithm, but is not optimal. Lots
of hard problems have greedy heuristics that work well in practice.)

Greedy problems come in a wide variety, which is why they’re hard to define.
Additionally, the “trick” to these problems usually involves figuring out some
mathematical proof showing that yes indeed, doing the same greedy action each
time will lead to a good solution. See what we did above to prove that this
looting algorithm works? That was pretty much a proof by contradiction. The
term “greedy” just generally encapsulates all algorithms that use some unique
trick to solve a problem, so the challenge with greedy algorithm problems is
figuring out what that trick is.

## Conclusion

|                         | **Examples of when to use it**                                                                      | **How to use it**                                                                                                               |
| ----------------------- | --------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------- |
| **Divide and conquer**  | - Sorting <br/> - Trees <br/> - Parsing                                                             | 1. Identify the base case. <br/> 2. Recursively split the input and solve each section. <br/> 3. Merge the solved splits.       |
| **Dynamic programming** | - Maximizing or minimizing <br/> - Subsequence <br/> - Edit distance <br/> - Some knapsack problems | 1. Identify the base case. <br/> 2. Identify the recurrence relation. <br/> 3. Memoize the results of each step of the problem. |
| **Greedy**              | - Pathfinding <br/> - Forming trees <br/> - Interval scheduling                                     | 1. Find a clever trick that allows for a locally optimal decision to lead to a globally optimal solution.                       |

I hope this has proved to be an enlightening experience. The examples gone over
here are incredibly simple, so I hope that this can form a starting point if you’re
hearing about these for the first time, or provides a refresher if you haven’t. I
believe that there isn’t an easy trick to destroy every technical test and online
assessment, so that means the next step is literally just to grind LeetCode. You
may find that there are a lot of techniques and tricks that weren’t covered here,
which is fine, as there are many ways to make these types of problems harder. At
the very least, every problem on there has a good explanation in the comments
showing how to solve it, which should provide some resources to make the more
advanced techniques easier to learn.
